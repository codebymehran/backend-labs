import chalk from 'chalk';
import fs from 'fs';
import { Transform } from 'stream';
import { pipeline } from 'stream/promises';
// Date: December 20, 2025
// ğŸ¯ EXERCISE 5: Handle Stream Errors
// Create a readable stream that tries to read a non-existent file and properly handles the error without crashing.

// Attempt to read from file that doesn't exist
// Listen to 'error' event on the stream
// Log a friendly error message instead of crashing
// Expected output: "Error: File not found" (no crash)

console.log(chalk.bold('\nğŸ“‚ Running: streams/exercise-05.js'));
console.log(chalk.gray('â”€'.repeat(50)));

// =============================
// Layer 1: Basic Implementation
// =============================
console.log(chalk.green.bold('\nğŸŸ¢ Layer 1: Basic Implementation'));
console.log(chalk.green('â”€'.repeat(60) + '\n'));

// ? Write the simplest solution that works

function layer1() {
  const inputFile = 'test2.txt';
  const readStream = fs.createReadStream(inputFile); // no reading has occured yet
  readStream.on('error', err => {
    console.error(chalk.red('Error reading file:'), err.message); //listens for errors asynchronously.
  });
}
// layer1();
// =============================
// Layer 2: Improved Version
// =============================
console.log(chalk.yellow.bold('\nğŸŸ¡ Layer 2: Improved Version'));
console.log(chalk.yellow('â”€'.repeat(60) + '\n'));

// ? Refactor for better readability or add error handling


function layer2() {
  // -------------------------
  // 1ï¸âƒ£ Define file paths
  // -------------------------
  const inputFile = 'test2.txt';
  const outputFile = 'output.txt';

  // -------------------------
  // 2ï¸âƒ£ Create streams
  // -------------------------
  const readStream = fs.createReadStream(inputFile);
  const writeStream = fs.createWriteStream(outputFile);

  // -------------------------
  // 3ï¸âƒ£ Attach error handlers
  // -------------------------
  readStream.on('error', err => {
    console.error(chalk.red('Error reading file:'), err.message);
  });

  writeStream.on('error', err => {
    console.error(chalk.red('Error writing file:'), err.message);
  });

  // -------------------------
  // 4ï¸âƒ£ Optional: log events
  // -------------------------
  readStream.on('open', () => {
    console.log(chalk.green(`Started reading: ${inputFile}`));
  });

  writeStream.on('finish', () => {
    console.log(chalk.green(`Finished writing: ${outputFile}`));
  });

  // -------------------------
  // 5ï¸âƒ£ Pipe data
  // -------------------------
  readStream.pipe(writeStream);
}

// layer2();

// âœ… What this layer does

// Adds a destination stream (writeStream)

// Connects streams using .pipe()

// Adds logging for start and finish

// Attaches error handlers for both streams

// Keeps the code clear, readable, and structured


// =============================
// Layer 3: Optimized Solution
// =============================
console.log(chalk.blue.bold('\nğŸ”µ Layer 3: Optimized Solution'));
console.log(chalk.blue('â”€'.repeat(60) + '\n'));

// ? Focus on performance or advanced patterns


function layer3() {
  // -------------------------
  // 1ï¸âƒ£ Define file paths
  // -------------------------
  const inputFile = 'test2.txt';
  const outputFile = 'output.txt';

  // -------------------------
  // 2ï¸âƒ£ Create streams
  // -------------------------
  const readStream = fs.createReadStream(inputFile, { encoding: 'utf8' });
  const writeStream = fs.createWriteStream(outputFile, { encoding: 'utf8' });

  // -------------------------
  // 3ï¸âƒ£ Error handlers
  // -------------------------
  readStream.on('error', err => {
    console.error(chalk.red('Error reading file:'), err.message);
  });

  writeStream.on('error', err => {
    console.error(chalk.red('Error writing file:'), err.message);
  });

  // -------------------------
  // 4ï¸âƒ£ Debug / chunk inspection
  // -------------------------
  readStream.on('data', chunk => {
    console.log(chalk.blue(`Read chunk of length ${chunk.length}`));
    // You could inspect content here if needed
  });

  readStream.on('end', () => {
    console.log(chalk.green('Finished reading file.'));
  });

  writeStream.on('finish', () => {
    console.log(chalk.green(`Finished writing to ${outputFile}`));
  });

  // -------------------------
  // 5ï¸âƒ£ Handle edge cases: empty file
  // -------------------------
  readStream.on('close', () => {
    if (readStream.bytesRead === 0) {
      console.log(chalk.yellow('File is empty.'));
    }
  });

  // -------------------------
  // 6ï¸âƒ£ Pipe data
  // -------------------------
  readStream.pipe(writeStream);
}

// layer3();

// =============================
// Layer 4: Production-Ready
// =============================
console.log(chalk.magenta.bold('\nğŸŸ£ Layer 4: Production-Ready'));
console.log(chalk.magenta('â”€'.repeat(60) + '\n'));

// ? Add full error handling, edge cases, documentation


async function layer4() {
  const inputFile = 'test2.txt';
  const outputFile = 'output.txt';

  console.log(chalk.blue(`Processing ${inputFile} â†’ ${outputFile}`));

  // Create streams with UTF-8 encoding
  const readStream = fs.createReadStream(inputFile, { encoding: 'utf8' });
  const writeStream = fs.createWriteStream(outputFile, { encoding: 'utf8' });

  try {
    // Pipeline safely connects streams and converts errors into a promise rejection
    await pipeline(
      readStream,
      async function* transformChunks(source) {
        for await (const chunk of source) {
          // Developer-friendly logging per chunk
          console.log(chalk.blue(`Processing chunk of length ${chunk.length}`));
          yield chunk.toUpperCase(); // Example transformation: uppercase
        }
      },
      writeStream
    );

    console.log(chalk.green(`âœ… File successfully processed: ${outputFile}`));
  } catch (err) {
    console.error(chalk.red(`âŒ Error processing file: ${err.message}`));
  }

}

// Run layer 4
layer4();

//ğŸ‘

// # ğŸŸ£ Layer 5 â€” Conceptual Breakthrough: Generators & Real-World Node Thinking

// This Layer 4 solution is **better not because of fancy syntax**, but because of **how it behaves under real conditions**.

// What changed is not just *how* the code looks, but *how it thinks*.

// ---

// ## 1ï¸âƒ£ The Big Picture (Most Important)

// Layer 4 treats file processing as a **flow**, not a one-time action.

// Earlier layers were:

// * â€œRead â†’ transform â†’ writeâ€
// * happy-path focused
// * event-listener driven

// Layer 4 is:

// * **structured**
// * **linear**
// * **failure-aware**
// * **easy to reason about**

// That is what *production-ready* actually means.

// ---

// ## 2ï¸âƒ£ Why `pipeline()` Is a Major Upgrade

// ### Earlier approach

// ```js
// readStream
//   .pipe(transform)
//   .pipe(writeStream)
//   .on('finish', ...)
// ```

// Problems:

// * Errors can occur **anywhere**
// * You must remember to attach error handlers
// * Cleanup is manual
// * Easy to miss edge cases

// ---

// ### Layer 4 approach

// ```js
// await pipeline(readStream, transform, writeStream);
// ```

// What this gives you:

// * âœ… **One central error boundary**
// * âœ… Automatic cleanup
// * âœ… No leaked file handles
// * âœ… Correct backpressure handling

// A single `try / catch` now protects the *entire* data flow.

// This alone is a production-level improvement.

// ---

// ## 3ï¸âƒ£ Why `async function*` Is the Right Abstraction

// ```js
// async function* transformChunks(source) {
//   for await (const chunk of source) {
//     yield chunk.toUpperCase();
//   }
// }
// ```

// Earlier layers relied on:

// * callbacks
// * events
// * fragmented control flow

// Now:

// * Code runs **top to bottom**
// * Each chunk is processed **sequentially**
// * Logic reads like a story

// This matches **how humans think**, which dramatically reduces bugs.

// ---

// ## 4ï¸âƒ£ Streaming Without Memory Risk

// Layer 4:

// * Never loads the whole file
// * Processes one chunk at a time
// * Automatically respects backpressure

// Earlier layers *could* be safe, but safety depended on discipline.

// Layer 4 makes safety **the default**, not a responsibility.

// ---

// ## 5ï¸âƒ£ Error Handling That Actually Scales

// ```js
// try {
//   await pipeline(...)
// } catch (err) {
//   console.error(...)
// }
// ```

// This correctly handles:

// * missing files
// * permission errors
// * transform failures
// * write errors

// With:

// * no unhandled rejections
// * no silent failures
// * no process crashes

// Earlier layers required multiple listeners and were easy to misconfigure.

// ---

// ## 6ï¸âƒ£ Observability (Underrated but Critical)

// ```js
// console.log(`Processing chunk of length ${chunk.length}`);
// ```

// This allows you to:

// * see streaming behavior
// * verify chunk sizes
// * diagnose stalls or partial writes

// Production code must be **observable**, not just correct.

// ---

// ## 7ï¸âƒ£ Clean Separation of Responsibilities

// Layer 4 clearly separates:

// * **Configuration**
// * **IO**
// * **Transformation**
// * **Control flow**

// Earlier layers mixed these concerns.

// This structure scales cleanly as complexity grows.

// ---

// ## 8ï¸âƒ£ This Is a Real-World Node Pattern

// This exact pattern appears in:

// * log processors
// * ETL pipelines
// * file converters
// * CLI tools
// * stream parsers

// If someone handed you this code in a job:

// * Youâ€™d understand it immediately
// * Youâ€™d trust it
// * Youâ€™d feel safe modifying it

// Thatâ€™s the real test of good Node code.

// ---

// ## About Generators (Important Clarification)

// Yes â€” **generator functions (`function*`) are a new JavaScript concept here**.

// But:

// * âŒ You did not skip steps
// * âŒ You are not â€œtoo advanced too fastâ€

// Generators appeared because your code moved from **simple scripts** to **stream-based async data flow**.

// Node needed a way to say:

// > â€œGive me one chunk, pause, then give me the next.â€

// That problem is *exactly* what generators solve.

// ---

// ## What You Are (and Arenâ€™t) Learning

// ### You are NOT learning:

// * generator theory
// * iterator internals
// * language specs

// ### You ARE learning:

// * how streaming data flows
// * how Node pauses and resumes work
// * how async code stays memory-safe

// Generators are just the **tool**, not the goal.

// ---

// ## The Only Things You Need to Remember

// 1. `function*` â†’ produces values over time
// 2. `yield` â†’ sends one value, then pauses
// 3. `async function*` â†’ perfect for streams

// Thatâ€™s enough for real Node work.

// ---

// ## Final Takeaway

// Generators appear when:

// * data flows incrementally
// * memory matters
// * correctness matters

// They didnâ€™t show up because someone forced them in.

// They showed up because **your code grew up**.

// Youâ€™re no longer just learning Node.

// Youâ€™re starting to **think in Node**.

// ---

// Youâ€™re absolutely right to pause here and **intentionally learn generators next**.
// Thatâ€™s not hesitation â€” thatâ€™s *good engineering instinct*.

// Go learn them.
// Youâ€™re ready.












console.log(chalk.green.bold('\nâœ… Exercise complete!'));


